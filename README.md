# UniLLM-TS

> ç»Ÿä¸€çš„ TypeScript LLM è°ƒç”¨åº“ï¼Œæ”¯æŒå¤šä¸ªä¸»æµå¤§è¯­è¨€æ¨¡å‹æä¾›å•†

[![npm version](https://img.shields.io/npm/v/unillm-ts.svg)](https://www.npmjs.com/package/unillm-ts)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue.svg)](https://www.typescriptlang.org/)

## ç‰¹æ€§

- ğŸš€ **è½»é‡çº§**ï¼šæ—  UIï¼Œä¸ä¾èµ–å¤–éƒ¨æœåŠ¡
- ğŸ”„ **ç»Ÿä¸€æ¥å£**ï¼šæä¾›ä¸€è‡´çš„å¯¹è¯è°ƒç”¨æ–¹å¼
- ğŸ”Œ **å¯æ‰©å±•**ï¼šæ”¯æŒæ–‡æœ¬å¯¹è¯ï¼Œåç»­å¯æ‰©å±•å…¶ä»–æ•°æ®æ ¼å¼
- ğŸ”’ **å®‰å…¨å­˜å‚¨**ï¼šæ”¯æŒåŠ å¯†å­˜å‚¨ API Keyï¼ˆä½¿ç”¨ keytarï¼‰
- ğŸ“¦ **æ˜“é›†æˆ**ï¼šä½œä¸º npm åŒ…ï¼Œä¸€è¡Œä»£ç å¼•å…¥
- âš™ï¸ **é…ç½®ç®¡ç†**ï¼šé€šè¿‡é…ç½®æ–‡ä»¶ç®¡ç† API Keyã€æ¨¡å‹ã€è¶…å‚ç­‰

## æ”¯æŒçš„æä¾›å•†

- âœ… OpenAI (GPT-4, GPT-3.5, etc.)
- âœ… é˜¿é‡Œäº‘é€šä¹‰åƒé—® (Qwen)
- âœ… æ™ºè°± AI (GLM-4)
- âœ… Moonshot AI (Kimi)
- âš ï¸ è®¯é£æ˜Ÿç« (éœ€è¦ WebSocket å®ç°)

## å®‰è£…

```bash
npm install unillm-ts
```

## å¿«é€Ÿå¼€å§‹

### 1. é…ç½® API Keys

é¦–å…ˆï¼Œä½¿ç”¨å®‰å…¨å­˜å‚¨è®¾ç½®æ‚¨çš„ API Keysï¼š

```typescript
import { setSecret } from 'unillm-ts';

// å­˜å‚¨ API Keys
await setSecret('openai-api-key', 'your-openai-key');
await setSecret('qwen-api-key', 'your-qwen-key');
await setSecret('zhipu-api-key', 'your-zhipu-key');
```

### 2. åˆ›å»ºé…ç½®æ–‡ä»¶

åˆ›å»º `llm_config.yaml` æ–‡ä»¶ï¼ˆæˆ–ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰ï¼š

```yaml
models:
  gpt-4:
    provider: openai
    model: gpt-4
    api_key: @secret:openai-api-key
    base_url: https://api.openai.com/v1

  qwen-plus:
    provider: qwen
    model: qwen-plus
    api_key: @secret:qwen-api-key

  glm-4:
    provider: zhipu
    model: glm-4
    api_key: @secret:zhipu-api-key

default_model: qwen-plus
debug: false
```

### 3. ä½¿ç”¨å•ä¾‹æ¨¡å¼ï¼ˆæ¨èï¼‰

```typescript
import llmManager from 'unillm-ts';

// åˆå§‹åŒ–
await llmManager.init();

// æŸ¥è¯¢æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
const models = llmManager.listModels();
console.log('Available models:', models);

// é€‰æ‹©æ¨¡å‹
llmManager.selectModel('qwen-plus');

// ç®€å•å¯¹è¯ï¼ˆéæµå¼ï¼‰
const response = await llmManager.chatSimple('ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±');
console.log(response);

// æµå¼å¯¹è¯
const stream = await llmManager.chatStream('å†™ä¸€é¦–è¯—');
for await (const chunk of stream) {
  process.stdout.write(chunk);
}
```

### 4. ä½¿ç”¨ç±»å®ä¾‹

```typescript
import { LLMManager } from 'unillm-ts';

const manager = new LLMManager();
await manager.init('./my-config.yaml');

// é«˜çº§å¯¹è¯æ¥å£
const response = await manager.chat({
  messages: [
    { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹' },
    { role: 'user', content: 'è¯·å¸®æˆ‘åˆ†æä¸€ä¸‹è¿™æ®µä»£ç ' }
  ],
  temperature: 0.7,
  max_tokens: 1000,
  stream: false
}, 'gpt-4');

if (!isStream(response)) {
  console.log(response.content);
  console.log('Usage:', response.usage);
}
```

## API æ–‡æ¡£

### LLMManager

#### `init(configPath?: string): Promise<void>`

åˆå§‹åŒ–ç®¡ç†å™¨ï¼ŒåŠ è½½é…ç½®æ–‡ä»¶ã€‚

#### `listModels(): string[]`

è·å–æ‰€æœ‰é…ç½®çš„æ¨¡å‹åç§°åˆ—è¡¨ã€‚

#### `getModelsInfo(): ModelInfo[]`

è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯åˆ—è¡¨ã€‚

```typescript
interface ModelInfo {
  name: string;
  provider: string;
  model: string;
}
```

#### `selectModel(modelName: string): void`

é€‰æ‹©å½“å‰ä½¿ç”¨çš„æ¨¡å‹ã€‚

#### `getCurrentModel(): string | null`

è·å–å½“å‰é€‰æ‹©çš„æ¨¡å‹åç§°ã€‚

#### `getModelConfig(modelName: string): Partial<ModelConfig> | null`

è·å–æŒ‡å®šæ¨¡å‹çš„é…ç½®ï¼ˆæ•æ„Ÿä¿¡æ¯å·²è„±æ•ï¼‰ã€‚

#### `chat(options: ChatCompletionOptions, modelName?: string): Promise<ChatCompletionResponse | AsyncGenerator<string>>`

ç»Ÿä¸€çš„å¯¹è¯æ¥å£ã€‚

```typescript
interface ChatCompletionOptions {
  messages: Message[];
  temperature?: number;
  max_tokens?: number;
  stream?: boolean;
  top_p?: number;
}

interface Message {
  role: 'user' | 'assistant' | 'system';
  content: string | MessageContent[];
}
```

#### `chatSimple(message: string, modelName?: string): Promise<string>`

ç®€åŒ–çš„éæµå¼å¯¹è¯æ¥å£ã€‚

#### `chatStream(message: string, modelName?: string): AsyncGenerator<string>`

ç®€åŒ–çš„æµå¼å¯¹è¯æ¥å£ã€‚

#### `getSupportedProviders(): string[]`

è·å–æ”¯æŒçš„æä¾›å•†åˆ—è¡¨ã€‚

### å®‰å…¨å­˜å‚¨

#### `setSecret(key: string, value: string): Promise<void>`

å­˜å‚¨æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚ API Keyï¼‰ã€‚

#### `getSecret(key: string): Promise<string | null>`

è·å–å­˜å‚¨çš„æ•æ„Ÿä¿¡æ¯ã€‚

## é…ç½®æ–‡ä»¶è¯´æ˜

é…ç½®æ–‡ä»¶ä½¿ç”¨ YAML æ ¼å¼ï¼Œæ”¯æŒä»¥ä¸‹å­—æ®µï¼š

```yaml
models:
  model-name:
    provider: openai  # æä¾›å•†åç§°
    model: gpt-4      # æ¨¡å‹åç§°
    api_key: @secret:key-name  # ä½¿ç”¨ @secret: å‰ç¼€å¼•ç”¨å®‰å…¨å­˜å‚¨çš„å¯†é’¥
    # å…¶ä»–æä¾›å•†ç‰¹å®šçš„é…ç½®...

default_model: model-name  # é»˜è®¤ä½¿ç”¨çš„æ¨¡å‹
debug: false  # æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
```

## æ‰©å±•æ€§

åº“è®¾è®¡ä¸ºå¯æ‰©å±•çš„ï¼Œæ”¯æŒï¼š

1. **æ·»åŠ æ–°çš„æä¾›å•†**ï¼šå®ç° `LLMProvider` æŠ½è±¡ç±»
2. **å¤šæ¨¡æ€è¾“å…¥**ï¼š`MessageContent` æ¥å£æ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡ã€æ–‡ä»¶ç­‰ç±»å‹
3. **è‡ªå®šä¹‰é…ç½®**ï¼šé…ç½®é¡¹æ”¯æŒä»»æ„æ‰©å±•å­—æ®µ

### æ·»åŠ æ–°æä¾›å•†ç¤ºä¾‹

```typescript
import { LLMProvider } from 'unillm-ts';
import { ChatCompletionOptions, ChatCompletionResponse } from 'unillm-ts';

export class MyCustomProvider extends LLMProvider {
  async chatCompletion(
    options: ChatCompletionOptions
  ): Promise<ChatCompletionResponse | AsyncGenerator<string>> {
    // å®ç°æ‚¨çš„æä¾›å•†é€»è¾‘
  }
}
```

## æ³¨æ„äº‹é¡¹

1. **keytar ä¾èµ–**ï¼šéœ€è¦ç³»ç»Ÿæ”¯æŒ keytarï¼ˆå¯èƒ½éœ€è¦é¢å¤–çš„ç³»ç»Ÿåº“ï¼‰
2. **è®¯é£æ˜Ÿç«**ï¼šç›®å‰éœ€è¦ WebSocket å®ç°ï¼Œè¯·å‚è€ƒå®˜æ–¹ SDK
3. **API å¯†é’¥å®‰å…¨**ï¼šå»ºè®®ä½¿ç”¨ `@secret:` å‰ç¼€å­˜å‚¨æ•æ„Ÿä¿¡æ¯

## å¼€å‘

```bash
# å®‰è£…ä¾èµ–
npm install

# æ„å»º
npm run build

# å¼€å‘æ¨¡å¼ï¼ˆwatchï¼‰
npm run dev
```

## è®¸å¯è¯

MIT

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

